{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOd0OoUDJ5rFF8HE+PsX+nQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qingyuan-wu/Praxis-III/blob/ML-model/classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plastics Classifier using PlastNet\n",
        "This program will take in a single image of a plastic and classify it as either PET or non-PET."
      ],
      "metadata": {
        "id": "9JiVXYYZcy0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Import Libraries and Mount Drive"
      ],
      "metadata": {
        "id": "9cNzvheKeXDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "nZM023PveV57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diqfscw8ediC",
        "outputId": "299b220d-fffa-4de0-d6a1-3b1a6520cb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Pre-process Input Image\n",
        "The trained CNN works on 1277x1277 images. Pre-process input image to have dimensions 1277x1277 pixels."
      ],
      "metadata": {
        "id": "lnPmVAhLc8M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyheif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iULa1PpPZ0DO",
        "outputId": "c6b6c357-1169-4187-be49-cd26c657356a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyheif\n",
            "  Downloading pyheif-0.7.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyheif) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.0->pyheif) (2.21)\n",
            "Installing collected packages: pyheif\n",
            "Successfully installed pyheif-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "25Rzw_sIb-FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p01hRLstcxCu"
      },
      "outputs": [],
      "source": [
        "def preprocess(img_path, out_path, out_height, out_width):\n",
        "    import pyheif\n",
        "\n",
        "    '''\n",
        "    resize images to out_height x out_width and convert from heic to jpg if necessary\n",
        "    Save .jpg image to out_path\n",
        "    '''\n",
        "    if \".HEIC\" in img_path:\n",
        "        # convert from heic to jpeg\n",
        "        heif_file = pyheif.read(img_path)\n",
        "        image = Image.frombytes(\n",
        "            heif_file.mode, \n",
        "            heif_file.size, \n",
        "            heif_file.data,\n",
        "            \"raw\",\n",
        "            heif_file.mode,\n",
        "            heif_file.stride,\n",
        "            )\n",
        "        image.save(out_path, \"JPEG\")\n",
        "        # resize to 1277 by 1277\n",
        "        image = Image.open(out_path)\n",
        "    else:\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "    new_image = image.resize((out_height, out_width))\n",
        "    new_image.save(out_path)\n",
        "\n",
        "    print(f\"original size {image.size}\")\n",
        "    print(f\"new size {new_image.size}\") \n",
        "    image.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Feed Image into our Trained Convolutional Neural Network, PlastNet"
      ],
      "metadata": {
        "id": "dzRoRBDDdWMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PlastNet(nn.Module):\n",
        "    name = \"PlastNet\"\n",
        "    def __init__(self):\n",
        "        # input dimensions: 1277x1277\n",
        "        super(PlastNet, self).__init__()\n",
        "        self.conv_layer_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 6), # 1272\n",
        "            nn.BatchNorm2d(16), # must be output channel of previous layer\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(8, 8) # 159\n",
        "        )\n",
        "        # now size 636x636*16\n",
        "        self.conv_layer_2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, 3), # 157\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2), # 78\n",
        "            nn.Dropout2d(p=0.05),\n",
        "        )\n",
        "        # now size 317*317*16\n",
        "        self.conv_layer_3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 3), # 76\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2), # 38\n",
        "        )\n",
        "        # now size 157*157*32\n",
        "        self.conv_layer_4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3), # 36\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2), # 18\n",
        "            nn.Dropout2d(p=0.05), \n",
        "        )\n",
        "        # now size 77*77*64\n",
        "        self.conv_layer_5 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, 3), # 16\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2), # 8\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(8*8*32, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer_1(x)\n",
        "        x = self.conv_layer_2(x)\n",
        "        x = self.conv_layer_3(x)\n",
        "        x = self.conv_layer_4(x)\n",
        "        x = self.conv_layer_5(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AGG6dwFHe9fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/gdrive/MyDrive/Praxis 3/Plastics-Tracking/CNN-Models/model_CNN_V0_bs32_lr0.001_epoch9\"\n",
        "\n",
        "model = PlastNet()\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtoH2O8xdiZH",
        "outputId": "8fd06d25-a9b0-4894-e1bc-7af112177f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PlastNet(\n",
              "  (conv_layer_1): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(6, 6), stride=(1, 1))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer_2): Sequential(\n",
              "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout2d(p=0.05, inplace=False)\n",
              "  )\n",
              "  (conv_layer_3): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_layer_4): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout2d(p=0.05, inplace=False)\n",
              "  )\n",
              "  (conv_layer_5): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_plastic(image_path):\n",
        "    '''\n",
        "    Take in a 1277x1277 rgb image and classify it as either PET or non-PET\n",
        "    Return: a 2-element torch array with probabiliy of PET and non-PET\n",
        "    '''\n",
        "    im = plt.imread(image_path)\n",
        "    im = np.transpose(im, [2,0,1])\n",
        "    im = torch.from_numpy(im)\n",
        "    im = im.type(torch.FloatTensor)\n",
        "    # normalize = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    # im = normalize(im)\n",
        "    im = torch.unsqueeze(im, 0)\n",
        "\n",
        "    pred = model(im)\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    pred = softmax(pred)\n",
        "    print(f\"PET: {pred[0][0]}, not PET: {pred[0][1]}\")\n",
        "    return pred"
      ],
      "metadata": {
        "id": "Sncw4NYK05bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_path = \"/content/gdrive/MyDrive/Praxis 3/Plastics-Tracking/Plastics-Images-Own/IMG_7342.HEIC\"\n",
        "out_path = \"/content/gdrive/MyDrive/Praxis 3/Plastics-Tracking/Plastics-Images-Own-Resized/NP_IMG_7342.jpg\"\n",
        "preprocess(source_path, out_path, 200, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s19EkrzlXAXY",
        "outputId": "e94615a8-c550-4575-d81c-bca8c4e32949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original size (3024, 3024)\n",
            "new size (200, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    wadaba_not_pet = \"/content/gdrive/MyDrive/Praxis 3/Plastics-Tracking/WaDaBa-Processed/02-NOT-PET/0007_a06b01c2d0e0f0g0h2.jpg\"\n",
        "    \n",
        "    # own_pet = \"/content/gdrive/MyDrive/Praxis 3/Plastics-Tracking/Plastics-Images-Own-Resized/IMG_7332.jpg\"\n",
        "    # own_not_pet = \"/content/gdrive/MyDrive/Praxis 3/Plastics-Tracking/Plastics-Images-Own-Resized/NP_IMG_7337.jpg\"\n",
        "    im = Image.open(out_path)\n",
        "    plt.imshow(im)\n",
        "    plt.axis('off')\n",
        "    plt.title(out_path.split(\"/\")[-1])\n",
        "    classify_plastic(out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inSSIChbf1ew",
        "outputId": "17226a44-480a-4cb7-f039-80e19fcc1810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PET: 0.0, not PET: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-PET Things **correctly** classified as non-PET\n",
        "- a bowl of soup (7344)\n",
        "- an orange t-shirt (7346)\n",
        "\n",
        "\n",
        "### Non-PET Things **incorrectly** classified\n",
        "- Doritos bag\n",
        "- shampoo bottle (HDPE instead of PET)\n",
        "- calculator\n",
        "- pencil case\n",
        "- ipad\n",
        "- battery\n",
        "- transparent tape case\n",
        "- stapler\n",
        "- tide detergent bottle"
      ],
      "metadata": {
        "id": "nT6Mrewzog3P"
      }
    }
  ]
}